{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rida-Zubair/Calculator/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPOWd0bC-yU-",
        "outputId": "e3999f1e-9ff1-4a41-98f5-d97c1f0f6d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "npm is already the newest version (8.5.1~ds-1).\n",
            "nodejs is already the newest version (12.22.9~dfsg-1ubuntu3.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "\u001b[K\u001b[?25h\n",
            "changed 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "(Reading database ... 133846 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.7.0) over (2025.7.0) ...\n",
            "Setting up cloudflared (2025.7.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cloudflared is already the newest version (2025.7.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas matplotlib seaborn plotly openai fpdf streamlit -q\n",
        "!pip install streamlit pandas matplotlib seaborn plotly openai fpdf -q\n",
        "!apt-get install nodejs npm -y\n",
        "!npm install -g localtunnel\n",
        "!pip install streamlit pandas matplotlib seaborn plotly openai fpdf python-dotenv -q\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "!pip install streamlit pandas matplotlib seaborn plotly openai fpdf -q\n",
        "!apt-get install cloudflared -y\n",
        "!pip install openai==0.28 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2wxDeOA0Xn",
        "outputId": "eae8b742-fda9-4172-fcf1-86003e9c9374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cloudflared is already the newest version (2025.7.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pandas matplotlib seaborn plotly fpdf torch transformers accelerate -q\n",
        "!apt-get install cloudflared -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzznxjHHJa8g",
        "outputId": "5730bc89-1719-4b04-e991-8950d69af162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mâš ï¸  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Rida Zubair` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Rida Zubair`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVGo9O0B_drD",
        "outputId": "e0f355e6-e9bf-4e58-bbd7-1b6b7981471a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”‘ Enter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass.getpass('ğŸ”‘ Enter your OpenAI API Key: ')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZDI6epk_jAo",
        "outputId": "26ac7845-7f9d-4800-bd33-a1fe8eb61cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting insightify_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile insightify_app.py\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF & CUDA warnings\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline\n",
        "from fpdf import FPDF\n",
        "import streamlit as st\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "\n",
        "# ===============================\n",
        "# Load Hugging Face Falcon model\n",
        "# ===============================\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return pipeline(\"text-generation\",\n",
        "                    model=\"tiiuae/falcon-7b-instruct\",\n",
        "                    device=-1)\n",
        "\n",
        "llama_model = load_model()\n",
        "\n",
        "# ===============================\n",
        "# Data Cleaning\n",
        "# ===============================\n",
        "def clean_data(df):\n",
        "    df = df.copy()\n",
        "    df = df.dropna(how='all')\n",
        "    df = df.fillna(0)\n",
        "    df.columns = [str(c).strip().replace(' ', '_') for c in df.columns]\n",
        "\n",
        "    # Convert all object columns to strings to avoid Arrow errors\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "    return df\n",
        "\n",
        "# ===============================\n",
        "# Enhanced Summary Generation\n",
        "# ===============================\n",
        "def summarize_data(df):\n",
        "    summary = pd.DataFrame()\n",
        "\n",
        "    # Basic numeric stats\n",
        "    numeric_summary = df.describe(include=[float, int]).transpose()\n",
        "    numeric_summary[\"median\"] = df.median(numeric_only=True)\n",
        "    numeric_summary[\"mode\"] = df.mode(numeric_only=True).iloc[0]\n",
        "    numeric_summary[\"missing_%\"] = df.isnull().mean() * 100\n",
        "    numeric_summary[\"skewness\"] = df.skew(numeric_only=True)\n",
        "    numeric_summary[\"kurtosis\"] = df.kurtosis(numeric_only=True)\n",
        "\n",
        "    # Combine all stats\n",
        "    summary = numeric_summary\n",
        "\n",
        "    return summary\n",
        "\n",
        "# ===============================\n",
        "# Extra Statistics Display\n",
        "# ===============================\n",
        "def extra_stats(df):\n",
        "    st.subheader(\"ğŸ” Data Type & Value Overview\")\n",
        "\n",
        "    col_info = pd.DataFrame({\n",
        "        \"Column\": df.columns,\n",
        "        \"Data Type\": df.dtypes.astype(str),\n",
        "        \"Unique Values\": df.nunique(),\n",
        "        \"Missing Values\": df.isnull().sum()\n",
        "    })\n",
        "    st.dataframe(col_info)\n",
        "\n",
        "    # Correlation heatmap\n",
        "    numeric_cols = df.select_dtypes(include='number')\n",
        "    if not numeric_cols.empty:\n",
        "        st.subheader(\"ğŸ“Š Correlation Heatmap\")\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "        st.pyplot(plt.gcf())\n",
        "        plt.close()\n",
        "\n",
        "    # Top 5 frequent categories for categorical columns\n",
        "    cat_cols = df.select_dtypes(include='object')\n",
        "    if not cat_cols.empty:\n",
        "        st.subheader(\"ğŸ·ï¸ Top Categories in Categorical Columns\")\n",
        "        for col in cat_cols.columns:\n",
        "            st.write(f\"**{col}**:\")\n",
        "            st.write(df[col].value_counts().head(5))\n",
        "\n",
        "# ===============================\n",
        "# Feature Importance (Quick Look)\n",
        "# ===============================\n",
        "def feature_importance(df, target_col):\n",
        "    numeric_cols = df.select_dtypes(include='number').drop(columns=[target_col], errors='ignore')\n",
        "    if numeric_cols.empty:\n",
        "        st.info(\"No numeric columns for feature importance calculation.\")\n",
        "        return\n",
        "\n",
        "    X = numeric_cols\n",
        "    y = df[target_col]\n",
        "\n",
        "    try:\n",
        "        if y.nunique() <= 10:  # Classification\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "            model.fit(X, y)\n",
        "            importances = model.feature_importances_\n",
        "        else:  # Regression\n",
        "            model = RandomForestRegressor(random_state=42)\n",
        "            model.fit(X, y)\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "        feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "        st.subheader(\"ğŸŒŸ Feature Importance (Quick Look)\")\n",
        "        st.bar_chart(feat_imp)\n",
        "    except Exception as e:\n",
        "        st.error(f\"âš ï¸ Feature importance calculation failed: {str(e)}\")\n",
        "\n",
        "# ===============================\n",
        "# Data Balance Check\n",
        "# ===============================\n",
        "def data_balance_check(df, target_col):\n",
        "    if target_col not in df.columns:\n",
        "        return\n",
        "    counts = df[target_col].value_counts()\n",
        "    st.subheader(\"âš–ï¸ Data Balance Check\")\n",
        "    st.bar_chart(counts)\n",
        "\n",
        "    min_ratio = counts.min() / counts.max()\n",
        "    if min_ratio < 0.5:\n",
        "        st.warning(\"âš ï¸ Warning: The dataset appears imbalanced. Consider using techniques like resampling.\")\n",
        "\n",
        "# ===============================\n",
        "# Generate Plots\n",
        "# ===============================\n",
        "def generate_plots(df):\n",
        "    figs = []\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.histplot(df[col], kde=True, ax=ax)\n",
        "        ax.set_title(f'Distribution of {col}')\n",
        "        figs.append(fig)\n",
        "    return figs\n",
        "\n",
        "# ===============================\n",
        "# AI Insights using Falcon\n",
        "# ===============================\n",
        "def generate_insights(df, business_type):\n",
        "    sample_data = df.head(5).to_dict()\n",
        "    prompt = f\"\"\"\n",
        "    You are a data analyst AI.\n",
        "    Analyze this dataset sample for a {business_type} business:\n",
        "    {sample_data}\n",
        "    Provide 5 main insights and recommendations in plain English.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llama_model(prompt,\n",
        "                               max_new_tokens=256,\n",
        "                               do_sample=True,\n",
        "                               temperature=0.7,\n",
        "                               truncation=True)\n",
        "        return response[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error generating insights: {str(e)}\"\n",
        "\n",
        "# ===============================\n",
        "# PDF Report Generation\n",
        "# ===============================\n",
        "def generate_pdf(summary, insights):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, \"Insightify Auto Report\\n\\nSummary:\\n\")\n",
        "    pdf.multi_cell(0, 10, summary.to_string())\n",
        "    pdf.multi_cell(0, 10, \"\\nInsights:\\n\" + insights)\n",
        "    pdf.output(\"report.pdf\")\n",
        "\n",
        "# ===============================\n",
        "# Main Streamlit App\n",
        "# ===============================\n",
        "def main():\n",
        "    st.title(\"Insightify â€“ Upload â†’ Analyze â†’ Understand â†’ Act (Open-Source)\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload your CSV file\", type=[\"csv\"])\n",
        "    business_type = st.selectbox(\"Select business type\", [\"General\", \"Sales\", \"HR\", \"Healthcare\", \"Finance\"])\n",
        "    target_col = st.text_input(\"Enter target column name (optional for balance & feature importance):\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        df = clean_data(df)\n",
        "        st.write(\"### Data Preview\", df.head())\n",
        "\n",
        "        summary = summarize_data(df)\n",
        "        st.write(\"### ğŸ“‹ Enhanced Data Summary\", summary)\n",
        "\n",
        "        extra_stats(df)\n",
        "\n",
        "        if target_col and target_col in df.columns:\n",
        "            data_balance_check(df, target_col)\n",
        "            feature_importance(df, target_col)\n",
        "\n",
        "        figs = generate_plots(df)\n",
        "        for fig in figs:\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)  # âœ… Prevent memory overflow\n",
        "\n",
        "        if st.button(\"Generate AI Insights\"):\n",
        "            insights = generate_insights(df, business_type)\n",
        "            st.write(\"### AI Insights (Falcon-7B)\", insights)\n",
        "            generate_pdf(summary, insights)\n",
        "            st.success(\"âœ… PDF report generated (report.pdf)\")\n",
        "\n",
        "        query = st.text_input(\"Ask a question about your dataset:\")\n",
        "        if query:\n",
        "            try:\n",
        "                prompt = f\"You are a data analyst. Based on this data:\\n{df.head(20).to_dict()}\\nQuestion: {query}\"\n",
        "                response = llama_model(prompt,\n",
        "                                       max_new_tokens=150,\n",
        "                                       do_sample=True,\n",
        "                                       temperature=0.7,\n",
        "                                       truncation=True)\n",
        "                st.write(response[0]['generated_text'])\n",
        "            except Exception as e:\n",
        "                st.error(f\"âš ï¸ Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfEuFDeEZToj",
        "outputId": "83d6058b-c2a8-4a70-84e5-b7f54d74e377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mâš ï¸  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Rida Zubair` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Rida Zubair`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgb0W8ucX9cv",
        "outputId": "5d8fe471-ae17-4bcf-c3b2-2d9a8d29b2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting insightify_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile insightify_app.py\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF & CUDA warnings\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline\n",
        "from fpdf import FPDF\n",
        "import streamlit as st\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "\n",
        "# ===============================\n",
        "# Load Hugging Face Falcon model\n",
        "# ===============================\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return pipeline(\"text-generation\",\n",
        "                    model=\"tiiuae/falcon-7b-instruct\",\n",
        "                    device=-1)\n",
        "\n",
        "llama_model = load_model()\n",
        "\n",
        "# ===============================\n",
        "# Data Cleaning\n",
        "# ===============================\n",
        "def clean_data(df):\n",
        "    df = df.copy()\n",
        "    df = df.dropna(how='all')\n",
        "    df = df.fillna(0)\n",
        "    df.columns = [str(c).strip().replace(' ', '_') for c in df.columns]\n",
        "\n",
        "    # Convert all object columns to strings to avoid Arrow errors\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "    return df\n",
        "\n",
        "# ===============================\n",
        "# Enhanced Summary Generation\n",
        "# ===============================\n",
        "def summarize_data(df):\n",
        "    summary = pd.DataFrame()\n",
        "\n",
        "    # Basic numeric stats\n",
        "    numeric_summary = df.describe(include=[float, int]).transpose()\n",
        "    numeric_summary[\"median\"] = df.median(numeric_only=True)\n",
        "    numeric_summary[\"mode\"] = df.mode(numeric_only=True).iloc[0]\n",
        "    numeric_summary[\"missing_%\"] = df.isnull().mean() * 100\n",
        "    numeric_summary[\"skewness\"] = df.skew(numeric_only=True)\n",
        "    numeric_summary[\"kurtosis\"] = df.kurtosis(numeric_only=True)\n",
        "\n",
        "    # Combine all stats\n",
        "    summary = numeric_summary\n",
        "\n",
        "    return summary\n",
        "\n",
        "# ===============================\n",
        "# Extra Statistics Display\n",
        "# ===============================\n",
        "def extra_stats(df):\n",
        "    st.subheader(\"ğŸ” Data Type & Value Overview\")\n",
        "\n",
        "    col_info = pd.DataFrame({\n",
        "        \"Column\": df.columns,\n",
        "        \"Data Type\": df.dtypes.astype(str),\n",
        "        \"Unique Values\": df.nunique(),\n",
        "        \"Missing Values\": df.isnull().sum()\n",
        "    })\n",
        "    st.dataframe(col_info)\n",
        "\n",
        "    # Correlation heatmap\n",
        "    numeric_cols = df.select_dtypes(include='number')\n",
        "    if not numeric_cols.empty:\n",
        "        st.subheader(\"ğŸ“Š Correlation Heatmap\")\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "        st.pyplot(plt.gcf())\n",
        "        plt.close()\n",
        "\n",
        "    # Top 5 frequent categories for categorical columns\n",
        "    cat_cols = df.select_dtypes(include='object')\n",
        "    if not cat_cols.empty:\n",
        "        st.subheader(\"ğŸ·ï¸ Top Categories in Categorical Columns\")\n",
        "        for col in cat_cols.columns:\n",
        "            st.write(f\"**{col}**:\")\n",
        "            st.write(df[col].value_counts().head(5))\n",
        "\n",
        "# ===============================\n",
        "# Feature Importance (Quick Look)\n",
        "# ===============================\n",
        "def feature_importance(df, target_col):\n",
        "    numeric_cols = df.select_dtypes(include='number').drop(columns=[target_col], errors='ignore')\n",
        "    if numeric_cols.empty:\n",
        "        st.info(\"No numeric columns for feature importance calculation.\")\n",
        "        return\n",
        "\n",
        "    X = numeric_cols\n",
        "    y = df[target_col]\n",
        "\n",
        "    try:\n",
        "        if y.nunique() <= 10:  # Classification\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "            model.fit(X, y)\n",
        "            importances = model.feature_importances_\n",
        "        else:  # Regression\n",
        "            model = RandomForestRegressor(random_state=42)\n",
        "            model.fit(X, y)\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "        feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "        st.subheader(\"ğŸŒŸ Feature Importance (Quick Look)\")\n",
        "        st.bar_chart(feat_imp)\n",
        "    except Exception as e:\n",
        "        st.error(f\"âš ï¸ Feature importance calculation failed: {str(e)}\")\n",
        "\n",
        "# ===============================\n",
        "# Data Balance Check\n",
        "# ===============================\n",
        "def data_balance_check(df, target_col):\n",
        "    if target_col not in df.columns:\n",
        "        return\n",
        "    counts = df[target_col].value_counts()\n",
        "    st.subheader(\"âš–ï¸ Data Balance Check\")\n",
        "    st.bar_chart(counts)\n",
        "\n",
        "    min_ratio = counts.min() / counts.max()\n",
        "    if min_ratio < 0.5:\n",
        "        st.warning(\"âš ï¸ Warning: The dataset appears imbalanced. Consider using techniques like resampling.\")\n",
        "\n",
        "# ===============================\n",
        "# Generate Plots\n",
        "# ===============================\n",
        "def generate_plots(df):\n",
        "    figs = []\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.histplot(df[col], kde=True, ax=ax)\n",
        "        ax.set_title(f'Distribution of {col}')\n",
        "        figs.append(fig)\n",
        "    return figs\n",
        "\n",
        "# ===============================\n",
        "# AI Insights using Falcon\n",
        "# ===============================\n",
        "def generate_insights(df, business_type):\n",
        "    sample_data = df.head(5).to_dict()\n",
        "    prompt = f\"\"\"\n",
        "    You are a data analyst AI.\n",
        "    Analyze this dataset sample for a {business_type} business:\n",
        "    {sample_data}\n",
        "    Provide 5 main insights and recommendations in plain English.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llama_model(prompt,\n",
        "                               max_new_tokens=100,\n",
        "                               do_sample=True,\n",
        "                               temperature=0.7,\n",
        "                               truncation=True)\n",
        "        return response[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error generating insights: {str(e)}\"\n",
        "\n",
        "# ===============================\n",
        "# PDF Report Generation\n",
        "# ===============================\n",
        "def generate_pdf(summary, insights):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, \"Insightify Auto Report\\n\\nSummary:\\n\")\n",
        "    pdf.multi_cell(0, 10, summary.to_string())\n",
        "    pdf.multi_cell(0, 10, \"\\nInsights:\\n\" + insights)\n",
        "    pdf.output(\"report.pdf\")\n",
        "\n",
        "# ===============================\n",
        "# Main Streamlit App\n",
        "# ===============================\n",
        "def main():\n",
        "    st.title(\"Insightify â€“ Upload â†’ Analyze â†’ Understand â†’ Act (Open-Source)\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload your CSV file\", type=[\"csv\"])\n",
        "    business_type = st.selectbox(\"Select business type\", [\"General\", \"Sales\", \"HR\", \"Healthcare\", \"Finance\"])\n",
        "    target_col = st.text_input(\"Enter target column name (optional for balance & feature importance):\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        df = clean_data(df)\n",
        "        st.write(\"### Data Preview\", df.head())\n",
        "\n",
        "        summary = summarize_data(df)\n",
        "        st.write(\"### ğŸ“‹ Enhanced Data Summary\", summary)\n",
        "\n",
        "        extra_stats(df)\n",
        "\n",
        "        if target_col and target_col in df.columns:\n",
        "            data_balance_check(df, target_col)\n",
        "            feature_importance(df, target_col)\n",
        "\n",
        "        figs = generate_plots(df)\n",
        "        for fig in figs:\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)  # âœ… Prevent memory overflow\n",
        "\n",
        "        if st.button(\"Generate AI Insights\"):\n",
        "            insights = generate_insights(df, business_type)\n",
        "            st.write(\"### AI Insights (Falcon-7B)\", insights)\n",
        "            generate_pdf(summary, insights)\n",
        "            st.success(\"âœ… PDF report generated (report.pdf)\")\n",
        "\n",
        "        query = st.text_input(\"Ask a question about your dataset:\")\n",
        "        if query:\n",
        "            try:\n",
        "                prompt = f\"You are a data analyst. Based on this data:\\n{df.head(20).to_dict()}\\nQuestion: {query}\"\n",
        "                response = llama_model(prompt,\n",
        "                                       max_new_tokens=150,\n",
        "                                       do_sample=True,\n",
        "                                       temperature=0.7,\n",
        "                                       truncation=True)\n",
        "                st.write(response[0]['generated_text'])\n",
        "            except Exception as e:\n",
        "                st.error(f\"âš ï¸ Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWlNAcHmJZxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpj0fuPLKGAa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XksL3djj_mxE",
        "outputId": "cfe1b1dd-b222-429f-c91e-b4a607ff9d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-07-31T13:05:49Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-07-31T13:05:49Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.169.141.122:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m |  https://fears-picnic-portsmouth-fate.trycloudflare.com                                    |\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: f0dc5a3c-4021-4150-8fa4-980d4f33c43e\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.27\n",
            "2025/07/31 13:05:55 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-07-31T13:05:55Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mf8a5e99c-1342-430c-94ee-cd056aa1632c \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.27 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n",
            "2025-07-31 13:06:27.596069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753967187.628285   11189 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753967187.637456   11189 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 21.76it/s]\n",
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "!streamlit run insightify_app.py --server.port 8501 & cloudflared tunnel --url http://localhost:8501\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtZgIGZ7MNKJIBGoDTehM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}